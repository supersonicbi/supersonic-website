<!doctype html><html lang=en-us><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="配置LLM # Chat Model # SuperSonic可以从两个粒度配置Chat Model：
1. 系统粒度 # 1.2.修改langchain4j.yaml配置文件方式 # 从0.9.4版本开始，langchain4j配置从application.yaml文件中单独抽取，方便用户配置；以langchain4j-local.yaml为例配置：
目前支持：open-ai、ollama两种方式； 1、open_ai 用于连接云端模型；通常情况下各云上模型会兼容open-ai协议 2、ollama 用于连接本地模型；由ollama适配不同的模型
注意 推荐使用open_ai、ollama；其他azure、qianfan、dashscope、zhipu方式理论上也支持，但是待验证； open-ai 配置方式：
langchain4j: open-ai: chat-model: base-url: ${OPENAI_API_BASE:demo} api-key: ${OPENAI_API_KEY:demo} model-name: ${OPENAI_MODEL_NAME:gpt-3.5-turbo} temperature: ${OPENAI_TEMPERATURE:0.0} timeout: ${OPENAI_TIMEOUT:PT60S} ollama 配置方式：
langchain4j: ollama: chat-model: base-url: ${OPENAI_API_BASE:demo} api-key: ${OPENAI_API_KEY:demo} model-name: ${OPENAI_MODEL_NAME:gpt-3.5-turbo} temperature: ${OPENAI_TEMPERATURE:0.0} timeout: ${OPENAI_TIMEOUT:PT60S} azure 配置方式：
langchain4j: azure-open-ai: chat-model: endpoint: https://xxx.openai.azure.com/ api-key: xxxx deployment-name: gpt-35-turbo max-tokens: 500 注意 以下方式配置，待验证： zhipu 配置方式：
langchain4j: zhipu: chat-model: base-url: base-url api-key: demo model-name: xxx temperature: 0."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://localhost:1313/docs/%E7%B3%BB%E7%BB%9F%E9%83%A8%E7%BD%B2/%E9%85%8D%E7%BD%AEllm/"><meta property="og:site_name" content="SuperSonic"><meta property="og:title" content="配置LLM"><meta property="og:description" content="配置LLM # Chat Model # SuperSonic可以从两个粒度配置Chat Model：
1. 系统粒度 # 1.2.修改langchain4j.yaml配置文件方式 # 从0.9.4版本开始，langchain4j配置从application.yaml文件中单独抽取，方便用户配置；以langchain4j-local.yaml为例配置：
目前支持：open-ai、ollama两种方式； 1、open_ai 用于连接云端模型；通常情况下各云上模型会兼容open-ai协议 2、ollama 用于连接本地模型；由ollama适配不同的模型
注意 推荐使用open_ai、ollama；其他azure、qianfan、dashscope、zhipu方式理论上也支持，但是待验证； open-ai 配置方式：
langchain4j: open-ai: chat-model: base-url: ${OPENAI_API_BASE:demo} api-key: ${OPENAI_API_KEY:demo} model-name: ${OPENAI_MODEL_NAME:gpt-3.5-turbo} temperature: ${OPENAI_TEMPERATURE:0.0} timeout: ${OPENAI_TIMEOUT:PT60S} ollama 配置方式：
langchain4j: ollama: chat-model: base-url: ${OPENAI_API_BASE:demo} api-key: ${OPENAI_API_KEY:demo} model-name: ${OPENAI_MODEL_NAME:gpt-3.5-turbo} temperature: ${OPENAI_TEMPERATURE:0.0} timeout: ${OPENAI_TIMEOUT:PT60S} azure 配置方式：
langchain4j: azure-open-ai: chat-model: endpoint: https://xxx.openai.azure.com/ api-key: xxxx deployment-name: gpt-35-turbo max-tokens: 500 注意 以下方式配置，待验证： zhipu 配置方式：
langchain4j: zhipu: chat-model: base-url: base-url api-key: demo model-name: xxx temperature: 0."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><title>配置LLM | SuperSonic</title>
<link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png><link rel=canonical href=http://localhost:1313/docs/%E7%B3%BB%E7%BB%9F%E9%83%A8%E7%BD%B2/%E9%85%8D%E7%BD%AEllm/><link rel=stylesheet href=/book.min.4964903a822a7acb10dac6d1ab524833c97fb5f99b141976bcb8a47d539be9c0.css integrity="sha256-SWSQOoIqessQ2sbRq1JIM8l/tfmbFBl2vLikfVOb6cA=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.85717f9df138a7d13258e735f836943b48090c90dd0953025a85ed3709b93b7b.js integrity="sha256-hXF/nfE4p9EyWOc1+DaUO0gJDJDdCVMCWoXtNwm5O3s=" crossorigin=anonymous></script></head><body><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>SuperSonic</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><a href=/docs/%E5%BF%AB%E9%80%9F%E4%BD%93%E9%AA%8C/>快速体验</a></li><li><a href=/docs/%E8%B4%A1%E7%8C%AE%E6%8C%87%E5%8D%97/>贡献指南</a></li><li><a href=/docs/%E9%A1%B9%E7%9B%AE%E6%9E%B6%E6%9E%84/>项目架构</a></li><li><span>系统部署</span><ul><li><a href=/docs/%E7%B3%BB%E7%BB%9F%E9%83%A8%E7%BD%B2/docker%E9%83%A8%E7%BD%B2/>Docker部署</a></li><li><a href=/docs/%E7%B3%BB%E7%BB%9F%E9%83%A8%E7%BD%B2/%E7%BC%96%E8%AF%91%E6%9E%84%E5%BB%BA/>编译构建</a></li><li><a href=/docs/%E7%B3%BB%E7%BB%9F%E9%83%A8%E7%BD%B2/%E9%85%8D%E7%BD%AEdb/>配置DB</a></li><li><a href=/docs/%E7%B3%BB%E7%BB%9F%E9%83%A8%E7%BD%B2/%E9%85%8D%E7%BD%AEllm/ class=active>配置LLM</a></li></ul></li><li><span>Chat BI</span><ul><li><a href=/docs/chat-bi/%E6%A6%82%E5%BF%B5/>概念</a></li><li><a href=/docs/chat-bi/%E9%85%8D%E7%BD%AE%E5%8A%A9%E7%90%86/>配置助理</a></li><li><a href=/docs/chat-bi/%E9%97%AE%E7%AD%94%E5%AF%B9%E8%AF%9D/>问答对话</a></li><li><a href=/docs/chat-bi/%E9%85%8D%E7%BD%AE%E6%8F%92%E4%BB%B6/>配置插件</a></li></ul></li><li><span>Headless BI</span><ul><li><a href=/docs/headless-bi/%E6%A6%82%E5%BF%B5/>概念</a></li><li><a href=/docs/headless-bi/%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93/>连接数据库</a></li><li><a href=/docs/headless-bi/%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8B/>构建模型</a></li><li><a href=/docs/headless-bi/%E5%88%9B%E5%BB%BA%E6%8C%87%E6%A0%87/>创建指标</a></li><li><a href=/docs/headless-bi/%E7%AE%A1%E7%90%86%E6%8C%87%E6%A0%87/>管理指标</a></li><li><a href=/docs/headless-bi/%E7%BB%84%E8%A3%85%E6%95%B0%E6%8D%AE%E9%9B%86/>组装数据集</a></li><li><a href=/docs/headless-bi/%E9%85%8D%E7%BD%AE%E6%95%B0%E6%8D%AE%E6%9D%83%E9%99%90/>配置数据权限</a></li><li><a href=/docs/headless-bi/%E6%A0%87%E7%AD%BE/>标签</a></li></ul></li><li><a href=/docs/chat-sdk%E9%9B%86%E6%88%90/>chat-sdk集成</a></li><li><a href=/docs/faq/>FAQ</a></li><li><a href=/docs/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E7%BD%AE/>系统设置</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>配置LLM</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#chat-model>Chat Model</a><ul><li><a href=#1-系统粒度><strong>1. 系统粒度</strong></a></li><li><a href=#2-助理粒度><strong>2. 助理粒度</strong></a></li></ul></li><li><a href=#embedding-model>Embedding Model</a></li><li><a href=#embedding-store>Embedding Store</a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=配置llm>配置LLM
<a class=anchor href=#%e9%85%8d%e7%bd%aellm>#</a></h1><h2 id=chat-model>Chat Model
<a class=anchor href=#chat-model>#</a></h2><p>SuperSonic可以从两个粒度配置Chat Model：</p><h3 id=1-系统粒度><strong>1. 系统粒度</strong>
<a class=anchor href=#1-%e7%b3%bb%e7%bb%9f%e7%b2%92%e5%ba%a6>#</a></h3><h4 id=12修改langchain4jyaml配置文件方式><strong>1.2.修改langchain4j.yaml配置文件方式</strong>
<a class=anchor href=#12%e4%bf%ae%e6%94%b9langchain4jyaml%e9%85%8d%e7%bd%ae%e6%96%87%e4%bb%b6%e6%96%b9%e5%bc%8f>#</a></h4><p>从0.9.4版本开始，langchain4j配置从application.yaml文件中单独抽取，方便用户配置；以langchain4j-local.yaml为例配置：</p><p><figure><img src=/img/supersonic_langchain4j_config.png#center></figure>目前支持：open-ai、ollama两种方式；
1、open_ai 用于连接云端模型；通常情况下各云上模型会兼容open-ai协议
2、ollama 用于连接本地模型；由ollama适配不同的模型</p><blockquote class="book-hint info"><strong>注意</strong>
推荐使用open_ai、ollama；其他azure、qianfan、dashscope、zhipu方式理论上也支持，但是待验证；</blockquote><p>open-ai 配置方式：</p><pre tabindex=0><code>langchain4j:
    open-ai:
        chat-model:
            base-url: ${OPENAI_API_BASE:demo}
            api-key: ${OPENAI_API_KEY:demo}
            model-name: ${OPENAI_MODEL_NAME:gpt-3.5-turbo}
            temperature: ${OPENAI_TEMPERATURE:0.0}
            timeout: ${OPENAI_TIMEOUT:PT60S}
</code></pre><p>ollama 配置方式：</p><pre tabindex=0><code>langchain4j:
    ollama:
        chat-model:
            base-url: ${OPENAI_API_BASE:demo}
            api-key: ${OPENAI_API_KEY:demo}
            model-name: ${OPENAI_MODEL_NAME:gpt-3.5-turbo}
            temperature: ${OPENAI_TEMPERATURE:0.0}
            timeout: ${OPENAI_TIMEOUT:PT60S}
</code></pre><p>azure 配置方式：</p><pre tabindex=0><code>langchain4j:
    azure-open-ai:
        chat-model:
          endpoint: https://xxx.openai.azure.com/
          api-key: xxxx
          deployment-name: gpt-35-turbo
          max-tokens: 500
</code></pre><p><blockquote class="book-hint info"><strong>注意</strong>
以下方式配置，待验证：</blockquote>zhipu 配置方式：</p><pre tabindex=0><code>langchain4j:
    zhipu:
        chat-model:
            base-url: base-url
            api-key: demo
            model-name: xxx
            temperature: 0.0
            timeout: PT60S
</code></pre><p>qianfan 配置方式：</p><pre tabindex=0><code>langchain4j:
    qianfan:
        chat-model:
            endpoint: endpoint
            api-key: demo
            deployment-name: xxx
            temperature: 0.0
            timeout: PT60S
</code></pre><p>dashscope 配置方式：</p><pre tabindex=0><code>langchain4j:
    dashscope:
        chat-model:
            endpoint: endpoint
            api-key: demo
            deployment-name: xxx
            temperature: 0.0
            timeout: PT60S
</code></pre><h3 id=2-助理粒度><strong>2. 助理粒度</strong>
<a class=anchor href=#2-%e5%8a%a9%e7%90%86%e7%b2%92%e5%ba%a6>#</a></h3><p>在助理管理模块，修改助理配置，填入相应的变量:
open-ai 配置方式：</p><figure><img src=/img/supersonic_agent_llm_1.png#center></figure><p>ollama 配置方式：</p><figure><img src=/img/supersonic_agent_llm_2.png#center></figure><h2 id=embedding-model>Embedding Model
<a class=anchor href=#embedding-model>#</a></h2><p>统一采用一种方式配置Embedding模型，支持in-memory、open-ai、zhipu、ollama、azure、qianfan、dashscope</p><figure><img src=/img/supersonic_embedding_model_new.png#center></figure><p>支持in-memory 配置方式：
目前支持bge-small-zh、all-minilm-l6-v2-q内嵌模型；并支持符合onnx格式的本地模型；</p><pre tabindex=0><code>langchain4j:
    in-memory:
        embedding-model:
            model-name: bge-small-zh
            modelPath: /data/model.onnx
            vocabularyPath: /data/onnx_vocab.txt
</code></pre><blockquote class="book-hint info"><p><strong>注意</strong>
如果是启动报错，version `GLIBC_2.27&rsquo; not found (required by xxxlibonnxruntime.so)，如下图所示：
原因是libonnxruntime.so库依赖的libm.so.6库的版本不匹配。libm.so.6是GNU C库（glibc）的一部分，而libonnxruntime.so需要的glibc版本是2.27，部分系统上的glibc版本不是2.27导致报错；</p><ul><li>1.可尝试切换open-ai、dashscope等提供的embedding-model；</li><li>2.最新master分支已修复；<br><a href=https://github.com/tencentmusic/supersonic/commit/93ea7a618c2e6cc268a47242934163cf456e9544>https://github.com/tencentmusic/supersonic/commit/93ea7a618c2e6cc268a47242934163cf456e9544</a></li><li>3.使用docker compose方式启动；</li></ul></blockquote><figure><img src=/img/supersonic_inmemory_error.jpg#center></figure><p>open-ai 配置方式：</p><pre tabindex=0><code>langchain4j:
    open-ai:
        embedding-model:
            base-url: ${OPENAI_API_BASE:demo}
            api-key: ${OPENAI_API_KEY:demo}
            model-name: xxx
            timeout: ${OPENAI_TIMEOUT:PT60S}
</code></pre><p>zhipu 配置方式：</p><pre tabindex=0><code>langchain4j:
    zhipu:
        embedding-model:
            base-url: ${OPENAI_API_BASE:demo}
            api-key: ${OPENAI_API_KEY:demo}
            model: xxx
</code></pre><p>ollama 配置方式：</p><pre tabindex=0><code>langchain4j:
    ollama:
        embedding-model:
            base-url: ${OPENAI_API_BASE:demo}
            model-name: xxx
            timeout: ${OPENAI_TIMEOUT:PT60S}
</code></pre><p>azure 配置方式：</p><pre tabindex=0><code>langchain4j:
    azure-open-ai:
        embedding-model:
          endpoint: https://xxxxx.openai.azure.com/
          api-key: 20f7b4
          deployment-name: text-embedding-ada-002
</code></pre><p>qianfan 配置方式：</p><pre tabindex=0><code>langchain4j:
    qianfan:
        embedding-model:
            base-url: ${OPENAI_API_BASE:demo}
            api-key: ${OPENAI_API_KEY:demo}
            model-name: xxx
</code></pre><p>dashscope 配置方式：</p><pre tabindex=0><code>langchain4j:
    dashscope:
        embedding-model:
            api-key: ${OPENAI_API_KEY:demo}
            model-name: xxx
</code></pre><p>其他的方式类似</p><h2 id=embedding-store>Embedding Store
<a class=anchor href=#embedding-store>#</a></h2><p>supsersonic支持in-memory、chroma、milvus三种模式；支持后续扩展更多Embedding Store。</p><p>in-memory 配置方式：</p><pre tabindex=0><code>langchain4j:
    in-memory:
        embedding-store:
            persist-path: /tmp
</code></pre><p>chroma 配置方式：</p><pre tabindex=0><code>langchain4j:
    chroma:
        embedding-store:
            baseUrl: http://0.0.0.0:8000
            timeout: 120s
</code></pre><p>milvus 配置方式：</p><pre tabindex=0><code>langchain4j:
    milvus:
        embedding-store:
            host: localhost
            port: 2379
            uri: http://0.0.0.0:2379
            token: demo
            dimension: 384
            timeout: 120s   
</code></pre><blockquote class="book-hint info"><p><strong>注意</strong>
如果是采用了in-memory embedding-store方式，如果修改了EmbeddingModel模型,需要清理一下本地数据：</p><pre tabindex=0><code>rm  /tmp/InMemory.*collection
rm  /tmp/InMemory.meta_collection
rm  /tmp/InMemory.preset_query_collection
rm  /tmp/InMemory.solved_query_collection
rm  /tmp/InMemory.text2dsl_agent_collection
</code></pre></blockquote></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#chat-model>Chat Model</a><ul><li><a href=#1-系统粒度><strong>1. 系统粒度</strong></a></li><li><a href=#2-助理粒度><strong>2. 助理粒度</strong></a></li></ul></li><li><a href=#embedding-model>Embedding Model</a></li><li><a href=#embedding-store>Embedding Store</a></li></ul></nav></div></aside></main></body></html>